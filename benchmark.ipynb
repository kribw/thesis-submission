{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"doc/predictions.txt\", sep=\"\\t\")\n",
    "\n",
    "# Columns from dataframe we want to benchmark\n",
    "configs = [\"llama3_base\", \"mistral_base\", \"llama3_ft\", \"mistral_ft\", \"llama3_base_rag1\", \"mistral_base_rag1\", \"llama3_base_rag2\", \"mistral_base_rag2\", \"llama3_ft_rag1\", \"mistral_ft_rag1\", \"llama3_ft_rag2\", \"mistral_ft_rag2\"]\n",
    "\n",
    "full_list = []\n",
    "partial_list = []\n",
    "error_code_list = []\n",
    "hallucination_list = []\n",
    "no_match_list = []\n",
    "\n",
    "# Benchmark each configuration\n",
    "for config in configs:\n",
    "    full = 0\n",
    "    partial = 0\n",
    "    error_code = 0\n",
    "    hallucination = 0\n",
    "    no_match = 0\n",
    "\n",
    "    def check_match(pred: str, gold: str) -> None:\n",
    "        global full, partial, error_code, hallucination, no_match\n",
    "\n",
    "        if pred == gold:\n",
    "            full +=1\n",
    "        elif pred[0:3] == gold[0:3]:\n",
    "            partial+=1\n",
    "        elif pred[0:5] == \"Æ99.9\":\n",
    "            error_code+=1\n",
    "        else:\n",
    "            hallucination+=1\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        prediction = re.search(r'[A-ZÆ]\\d{2}(?:\\.\\d)?', str(row[config]))\n",
    "        if(prediction):\n",
    "            gold = row[\"icd10\"]\n",
    "            code = prediction.group()\n",
    "            check_match(code, gold)\n",
    "        else:\n",
    "            # print(row[config])\n",
    "            no_match+=1\n",
    "            \n",
    "    full_list.append(full)\n",
    "    partial_list.append(partial)\n",
    "    error_code_list.append(error_code)\n",
    "    hallucination_list.append(hallucination)\n",
    "    no_match_list.append(no_match)\n",
    "\n",
    "    total = len(df)\n",
    "    print(f\"Config: {config}, Full: {full} ({(float(full)/total)*100:.0f}%), Partial: {partial} ({(float(partial)/total)*100:.0f}%), Error code: {error_code} ({(float(error_code)/total)*100:.0f}%), Hallucination: {hallucination} ({(float(hallucination)/total)*100:.0f}%)  No match: {no_match} ({(float(no_match)/total)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Setup data for dataframe\n",
    "data = {\n",
    "    \"Config\": [\"llama3_base\", \"mistral_base\", \"llama3_ft\", \"mistral_ft\",\n",
    "               \"llama3_base_rag1\", \"mistral_base_rag1\", \"llama3_base_rag2\", \"mistral_base_rag2\",\n",
    "               \"llama3_ft_rag1\", \"mistral_ft_rag1\", \"llama3_ft_rag2\", \"mistral_ft_rag2\"],\n",
    "    \"Full\": full_list,\n",
    "    \"Partial\": partial_list,\n",
    "    \"Error code\": error_code_list,\n",
    "    \"Hallucination\": hallucination_list,\n",
    "    \"No code\": no_match_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalize data to percentages\n",
    "df[\"Total\"] = df[\"Full\"] + df[\"Partial\"] + df[\"Error code\"] + df[\"Hallucination\"] + df[\"No code\"]\n",
    "df[\"Full\"] = df[\"Full\"] / df[\"Total\"] * 100\n",
    "df[\"Partial\"] = df[\"Partial\"] / df[\"Total\"] * 100\n",
    "df[\"Error code\"] = df[\"Error code\"] / df[\"Total\"] * 100\n",
    "df[\"Hallucination\"] = df[\"Hallucination\"] / df[\"Total\"] * 100\n",
    "df[\"No code\"] = df[\"No code\"] / df[\"Total\"] * 100\n",
    "\n",
    "# Config labels are very wide, need to shorten\n",
    "config_labels = [label.replace(\"_\", \"\\n\").replace(\"llama3\", \"L3\").replace(\"mistral\", \"M\")\n",
    "                 .replace(\"base\", \"B\").replace(\"ft\", \"FT\").replace(\"rag1\", \"R1\").replace(\"rag2\", \"R2\")\n",
    "                 for label in df[\"Config\"]]\n",
    "\n",
    "# Plot with shortened labels\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "df.set_index(\"Config\")[[\"Full\", \"Partial\", \"Error code\", \"Hallucination\", \"No code\"]].plot(\n",
    "    kind=\"bar\", stacked=True, ax=ax, color=[\"green\", \"yellow\", \"orange\", \"purple\", \"red\"])\n",
    "ax.set_ylabel(\"Percentage (%)\")\n",
    "ax.set_xlabel(\"Model and Configuration\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xticklabels(config_labels, rotation=0) \n",
    "plt.legend(title=\"Match Type\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.title(\"Model Predictions\")\n",
    "\n",
    "# Show plot with tight, compressed layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find average response length\n",
    "df = pd.read_csv(\"doc/predictions.txt\", sep=\"\\t\")\n",
    "\n",
    "configs = [\"llama3_base\", \"mistral_base\", \"llama3_ft\", \"mistral_ft\", \"llama3_base_rag1\", \"mistral_base_rag1\", \"llama3_base_rag2\", \"mistral_base_rag2\", \"llama3_ft_rag1\", \"mistral_ft_rag1\", \"llama3_ft_rag2\", \"mistral_ft_rag2\"]\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"{config}, Average length: {df[config].str.len().mean():.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
