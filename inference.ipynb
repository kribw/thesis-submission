{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftConfig, PeftModel\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# DEFINE CONSTANTS\n",
    "LLAMA3 = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "MISTRAL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "RAG1 = \"rag-sources/correct-test-predictions.pdf\"      # File containing all correct predictions\n",
    "RAG2 = \"rag-sources/icd10-tabular-list.pdf\"            # ICD-10 manual\n",
    "\n",
    "# Define which setup to run here\n",
    "model_name = MISTRAL     # LLAMA3, MISTRAL\n",
    "is_fine_tuned = False    # True, False\n",
    "rag_source = RAG2       # None, RAG1, RAG2\n",
    "input_file = \"doc/translations.txt\" # File to read dataframe from\n",
    "output_file = \"doc/test_results.txt\" # File to save updated dataframe to\n",
    "\n",
    "def get_model(name: str):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    global tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "    return AutoModelForCausalLM.from_pretrained(name,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\",\n",
    "                                             token=os.getenv(\"HF_TOKEN\"),\n",
    "                                             pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "# Get model based on defined setup\n",
    "if(is_fine_tuned):\n",
    "    adapter_path = \"llama-ft\" if model_name == LLAMA3 else \"mistral-ft\"\n",
    "    config = PeftConfig.from_pretrained(adapter_path)\n",
    "    model = get_model(config.base_model_name_or_path)\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "else:\n",
    "    model = get_model(model_name)\n",
    "\n",
    "# Set pad_token to suppress warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Ensure model is in eval mode for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_source is not None:\n",
    "    from langchain.document_loaders.pdf import PyPDFLoader\n",
    "    \n",
    "    loader = PyPDFLoader(rag_source)\n",
    "    pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_source is not None:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=0)\n",
    "    split_text = text_splitter.split_documents(pages)\n",
    "    print(f\"Split text into {len(split_text)} chunks.\")\n",
    "\n",
    "    # Print example chunk\n",
    "    split_text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_source is not None:\n",
    "    from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "\n",
    "    # Use embedding trained on medical data\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "    db = Chroma.from_documents(documents=split_text, embedding=embedding_function) #, persist_directory=\"chroma-db\"\n",
    "\n",
    "    query = \"Bronchitis\"\n",
    "    search_res = db.similarity_search(query, k=5)\n",
    "\n",
    "    # Print test result\n",
    "    print(search_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Manually fetch results from Chroma db\n",
    "# Concatenate and return as a single string\n",
    "def get_context(cause_of_death: str) -> str:\n",
    "    context = \"\"\n",
    "    search_res = db.similarity_search(cause_of_death)\n",
    "\n",
    "    for res in search_res:\n",
    "        context += f\"{res.page_content}\\n\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "def predict_mistral(prompt: str, cause_of_death: str) -> str:\n",
    "    prompt += f\"\\n\\nCause of death: {cause_of_death}\"\n",
    "    \n",
    "    if rag_source is not None:\n",
    "        context = get_context(cause_of_death)\n",
    "        prompt += f\"\\n\\nContext: {context}\"\n",
    "\n",
    "    inputs = tokenizer(f\"<s>[INST]{prompt}[/INST]\", return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(model.device), \n",
    "                            max_new_tokens=256, \n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7,\n",
    "                            top_p=0.9,\n",
    "                            )\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract output from [INST][/INST] tags\n",
    "    response = output.split(\"[/INST]\", 1)\n",
    "    res = response[-1].replace('\\n', ' ')\n",
    "    return res\n",
    "\n",
    "def predict_llama(prompt: str, cause_of_death: str) -> str:\n",
    "    user_input = f\"Cause of death: {cause_of_death}\"\n",
    "    \n",
    "    if rag_source is not None:\n",
    "        context = get_context(cause_of_death)\n",
    "        user_input += f\"\\n\\nContext: {context}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    raw_response = outputs[0][input_ids.shape[-1]:]\n",
    "    res = tokenizer.decode(raw_response, skip_special_tokens=True)\n",
    "    # Remove assistant tag from message\n",
    "    return res.strip().strip('assistant').replace(\"\\n\", \" \")\n",
    "\n",
    "def get_df_column() -> str:\n",
    "    col_name = \"\"\n",
    "    col_name += \"llama3\" if model_name == LLAMA3 else \"mistral\"\n",
    "    col_name += \"_ft\" if is_fine_tuned == True else \"_base\"\n",
    "    \n",
    "    if rag_source == RAG1:\n",
    "        col_name += \"_rag1\"\n",
    "    elif rag_source == RAG2:\n",
    "        col_name += \"_rag2\"\n",
    "\n",
    "    return col_name\n",
    "\n",
    "# Start timer to record runtime\n",
    "start = time.perf_counter()\n",
    "\n",
    "print(f\"RUNNING CONFIG: {get_df_column()}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# Read current data\n",
    "df = pd.read_csv(input_file, sep=\"\\t\")\n",
    "# df = df[:5]\n",
    "\n",
    "prompt = \"\"\"You are to assign an ICD-10 code to a cause of death using the following instructions:\n",
    "- Use standard ICD-10 codes, not ICD-10-CM billing codes.\n",
    "- Each ICD-10 code should be 3 or 5 characters long, for example: 'X01.0' or 'C15'.\n",
    "- If the cause of death is 'unknown' or 'blank', use code 'R99'.\n",
    "- If you lack sufficient information to assign a code, do not try to guess. Instead, use code 'Ã†99.9'.\n",
    "- Your response should only contain a single ICD-10 code using this format: '<ICD-10 CODE>'.\n",
    "- Do not explain your answer.\"\"\"\n",
    "\n",
    "# Iterate through dataset and set predictions\n",
    "for index, row in df.iterrows():  \n",
    "    try:\n",
    "        print(f\"Processing index {index} of {len(df)}.....\")\n",
    "\n",
    "        if(model_name == LLAMA3):\n",
    "            res = predict_llama(prompt, row[\"eng_translation\"])\n",
    "        elif(model_name == MISTRAL):\n",
    "            res = predict_mistral(prompt, row[\"eng_translation\"])\n",
    "        else:\n",
    "            raise Exception(\"Invalid setup at top of file.\")\n",
    "        \n",
    "        # Save benchmark for given model in dataframe\n",
    "        df.at[index, get_df_column()] = res\n",
    "\n",
    "        print(f\"Prediction: {res}, Gold: {row['icd10']}\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "# Save updated data as new file\n",
    "df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "time_elapsed = f\"Time elapsed: {end-start:.2f}\"\n",
    "print(time_elapsed)\n",
    "\n",
    "# Write compute time to file\n",
    "with open(\"timer.txt\", \"a\") as f:\n",
    "    f.write(f\"{get_df_column()} {time_elapsed}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print updated dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
